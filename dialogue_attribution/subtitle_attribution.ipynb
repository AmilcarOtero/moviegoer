{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# subtitle_attribution\n",
    "Subtitle attribution is the ultimate goal of this module. By using various clues in the audio, video, and subtitle tracks, we were able to match onscreen characters with voices. Now we just need to go through the subtitle file and attribute each line of dialogue to the appropriate character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dialogue_attribution_io import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with Actual Audio\n",
    "We can compare the subtitles with the audio in a scene we've previously identified. This specific .wav file was extracted from the audio track and only contains audio from this scene. In another file, we've explored the `speaker_diarization()` function from the `pyAudioAnalysis` library to split the audio from a conversation into two clusters (corresponding to each speaker)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = ('../extracted_audio/prison_stereo.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "clusters = pyAudioAnalysis.audioSegmentation.speaker_diarization(audio_file, 2, mid_window=.8, mid_step=0.1, short_window=0.02, lda_dim=0, plot_res=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "580"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clusters) # 580 clusters means 58 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speaker Changes\n",
    "We have to work through three challenges.\n",
    "1. pyAudioAnalysis' `speaker_diarization` doesn't actually tell us when someone is speaking, only the character who last spoke (whether currently speaking ot not). \n",
    "2. Frame extraction and subtitle timestamps may not line up perfectly. This is due to the issue of extracting frames from a 23.976p video.\n",
    "3. Subtitles and onscreen visuals usually don't align perfectly. Subtitle authoring is more of an art than a science. They usually appear onscreen before a character actually starts speaking, but sometimes (for dramatic effect or to avoid spoiling a joke), they appear just as the character speaks.\n",
    "\n",
    "Here are the speaker changes identified from the first 30 seconds of `speaker_diarization`. The `clusters` object contains the current or last-to-speak speaker, and reports this ten times per second. I've manually printed eight speaker changes (when the reported speaker flips from cluster 1 to 0 or 0 to 1). Remember that each item in the `clusters` list represents a tenth of a second. So a speaker change at `clusters[13]` means there's a speaker change 1.3 seconds into the scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flip A: 1.3 seconds\n",
      "0.0 12\n",
      "1.0 13\n",
      "\n",
      "Flip B: 2.9 seconds\n",
      "1.0 28\n",
      "0.0 29\n",
      "\n",
      "Flip C: 4.5 seconds\n",
      "0.0 43\n",
      "1.0 44\n",
      "\n",
      "Flip D: 14.6 seconds\n",
      "1.0 147\n",
      "0.0 148\n",
      "\n",
      "Flip E: 20.1 seconds\n",
      "0.0 200\n",
      "1.0 201\n",
      "\n",
      "Flip F: 25.3 seconds\n",
      "1.0 252\n",
      "0.0 253\n",
      "\n",
      "Flip G: 27.4 seconds\n",
      "0.0 273\n",
      "1.0 274\n",
      "\n",
      "Flip H: 29.6 seconds\n",
      "1.0 295\n",
      "0.0 296\n"
     ]
    }
   ],
   "source": [
    "# illustrate flip from 0 to 1 or vice versa\n",
    "print('Flip A: 1.3 seconds')\n",
    "for x in range(12, 14): # frame 13, or 1.3 seconds into the scene\n",
    "    print(clusters[x], x)\n",
    "print()\n",
    "print('Flip B: 2.9 seconds')\n",
    "for x in range(28, 30): # frame 29\n",
    "    print(clusters[x], x)\n",
    "print()\n",
    "print('Flip C: 4.4 seconds')\n",
    "for x in range(43, 45): # frame 44\n",
    "    print(clusters[x], x)\n",
    "print()\n",
    "print('Flip D: 14.8 seconds')\n",
    "for x in range(147, 149): # frame 148\n",
    "    print(clusters[x], x)\n",
    "print()\n",
    "print('Flip E: 20.1 seconds')\n",
    "for x in range(200, 202): # frame 201\n",
    "    print(clusters[x], x)\n",
    "print()\n",
    "print('Flip F: 25.3 seconds')\n",
    "for x in range(252, 254): # frame 253\n",
    "    print(clusters[x], x)\n",
    "print()\n",
    "print('Flip G: 27.4 seconds')\n",
    "for x in range(273, 275): # frame 274\n",
    "    print(clusters[x], x)\n",
    "print()\n",
    "print('Flip H: 29.6 seconds')\n",
    "for x in range(295, 297): # frame 296\n",
    "    print(clusters[x], x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cluster flip searching will be be defined as a function in `dialogue_attribution_io.py` called `get_cluster_changes()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_clust = 200\n",
    "index = 0\n",
    "cluster_change_indices = []\n",
    "\n",
    "for clust in clusters:\n",
    "    if clust != prev_clust and index != 0:\n",
    "        cluster_change_indices.append(index)\n",
    "    prev_clust = clust\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13, 29, 44, 148, 201, 253, 274, 296]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_change_indices[0:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example scene, I've manually looked at the .srt file and transcribed the speaker for the first 30 seconds. Since the audio file is only from this scene (and not the entire film), it start at 00:00:00; I've subtracted 00:12:46 from the subtitle times and wrote when they occur in the audio file. I've added commentary to how the 0-1 flips in `clusters` compare.\n",
    "\n",
    "`Helen Mirren:  12:47.7-12:49.1    :01.7-:03.1` A: Flips .4 seconds early\n",
    "\n",
    "`Jason Statham: 12:49.1-12:51.1    :03.1-:05.1` B: Flips .2 seconds early\n",
    "\n",
    "`Helen Mirren:  12:51.1-13:00.6    :05.1-:14.6` C: Flips .6 seconds early\n",
    "\n",
    "`Jason Statham: 13:00.6-13:02.3    :14.6-:16.3` D: Flips exactly on time\n",
    "\n",
    "`No subtitle:   13:02.3-13:05.2    :16.3-:19.2` (Correctly) doesn't flip\n",
    "\n",
    "`Helen Mirren:  13:05.2-13:12.1    :19.2-:26.1` E: Flips .9 seconds late\n",
    "\n",
    "`Jason Statham: 13:12.1-13:13.7    :26.1-:27.7` F: Flips .8 seconds early\n",
    "\n",
    "`Helen Mirren:  13:13.7-13:15.7    :27.7-:29.7` G: Flips .8 seconds early\n",
    "\n",
    "`Simulteanous:  13:15.7-13:17.7    :29.7-:31.7` H: Flips .1 seconds early, but the subtitles here are ambigious since they contain one line of dialogue for each character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the `clusters` from `voice_diarization()` to the actual subtitles, it looks like the clusters indicate speaker changes a little bit earlier than the subtitles. There is one exception, Flip E, which comes in .9 seconds late - this is probably because Helen Mirren's line starts with a curt and quiet \"well\", before continuing with the rest of her dialogue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subtitle Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_file = '../subtitles/hobbs_shaw.srt'\n",
    "subs = load_subtitles(sub_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to calculate the times of each cluster flip. As a reminder, the `pysrt` library works with `time` objects, not `datetime` objects. First we'll calculate the time of the first frame into a `time` object using the previously-defined `frame_to_time()` function. We'll then convert that into a `datetime` object, so we can conduct arithemetic operation (not possible on time objects. Then, we'll use each cluster flip as a `timedelta`, and add it to the scene start time.  \n",
    "\n",
    "An example for one cluster flip is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.time(0, 12, 46)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scene_start_time = frame_to_time(766)\n",
    "scene_start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_start_datetime = datetime.datetime(year=2000, month=1, day=1, hour=scene_start_time.hour, minute=scene_start_time.minute, second=scene_start_time.second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_change_indices[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each item in `clusters` represents a tenth of a second, we divide the cluster change index by ten, to be used as a `timedelta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.timedelta(seconds=1, microseconds=300000)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_flip = datetime.timedelta(seconds=cluster_change_indices[0] / 10)\n",
    "cluster_flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2000, 1, 1, 0, 12, 47, 300000)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flip_datetime = scene_start_datetime + cluster_flip\n",
    "flip_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.time(0, 12, 47, 300000)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flip_time = datetime.time(hour=flip_datetime.hour, minute=flip_datetime.minute, second=flip_datetime.second, microsecond=flip_datetime.microsecond)\n",
    "flip_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform this for all the cluster flips, though we're only interested in the first 9, the first 30 seconds' worth. This calculation of all the times when the speaker changes will be be defined as a function in `dialogue_attribution_io.py` called `get_subtitle_flip_times()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_start_time = frame_to_time(766)\n",
    "flip_times = []\n",
    "for cluster_change in cluster_change_indices:\n",
    "    scene_start_datetime = datetime.datetime(year=2000, month=1, day=1, hour=scene_start_time.hour, minute=scene_start_time.minute, second=scene_start_time.second)\n",
    "    cluster_flip = datetime.timedelta(seconds=cluster_change / 10)\n",
    "    flip_datetime = scene_start_datetime + cluster_flip\n",
    "    flip_time = datetime.time(hour=flip_datetime.hour, minute=flip_datetime.minute, second=flip_datetime.second, microsecond=flip_datetime.microsecond)\n",
    "    flip_times.append(flip_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.time(0, 12, 47, 300000),\n",
       " datetime.time(0, 12, 48, 900000),\n",
       " datetime.time(0, 12, 50, 400000),\n",
       " datetime.time(0, 13, 0, 800000),\n",
       " datetime.time(0, 13, 6, 100000),\n",
       " datetime.time(0, 13, 11, 300000),\n",
       " datetime.time(0, 13, 13, 400000),\n",
       " datetime.time(0, 13, 15, 600000)]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flip_times[0:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a list of all the times the speaker changes (according to the audio track), but as we realized above, this doesn't line up with the subtitles file.  These are the subtitles that we're looking for:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Helen Mirren:  12:47.7-12:49.1    :01.7-:03.1 A: Flips .4 seconds early, \"That's my boy.\"`\n",
    "\n",
    "`Jason Statham: 12:49.1-12:51.1    :03.1-:05.1 B: Flips .2 seconds early  \"No wonder we left\"`\n",
    "\n",
    "`Helen Mirren:  12:51.1-13:00.6    :05.1-:14.6 C: Flips .6 seconds early  \"She loves you, you know\"`\n",
    "\n",
    "`Jason Statham: 13:00.6-13:02.3    :14.6-:16.3 D: Flips exactly on time   \"Used to.\"`\n",
    "\n",
    "`No subtitle:   13:02.3-13:05.2    :16.3-:19.2 (Correctly) doesn't flip`\n",
    "\n",
    "`Helen Mirren:  13:05.2-13:12.1    :19.2-:26.1 E: Flips .9 seconds late   \"Well, one day, I just hope\"`\n",
    "\n",
    "`Jason Statham: 13:12.1-13:13.7    :26.1-:27.7 F: Flips .8 seconds early  \"How many years you got left?\"`\n",
    "\n",
    "`Helen Mirren:  13:13.7-13:15.7    :27.7-:29.7 G: Flips .8 seconds early  \"Two, with good behavior\"`\n",
    "\n",
    "`Simulteanous:  13:15.7-13:17.7    :29.7-:31.7 H: Ambiguous subtitles     \"-So how many, really?\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(laughs)\n",
      "\n",
      "That's my boy.\n",
      "\n",
      "No wonder we left\n",
      "the family business.\n",
      "\n",
      "Used to.\n",
      "\n",
      "Well, one day, I just hope\n",
      "that I walk through that door\n",
      "\n",
      "and I see the two of you\n",
      "sitting there.\n",
      "\n",
      "How many years you got left?\n",
      "\n",
      "Two, with good behavior.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# these are mostly incorrect\n",
    "for time in flip_times[0:8]:\n",
    "    print(subs.at(time).text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Offset\n",
    "We've previously defined functions that will allow us to add or subtract a time offset, to try and align the subtitles correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.time(0, 12, 47, 700000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flip_time = datetime.time(0, 12, 47, 300000)\n",
    "offset_time = add_time_offset(flip_time, 400)\n",
    "offset_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.time(0, 12, 46, 900000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flip_time = datetime.time(0, 12, 47, 300000)\n",
    "offset_time = subtract_time_offset(flip_time, 400)\n",
    "offset_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these, we can offset the time we search for subtitles. Here's an example of offsetting the first flip time by half a second, to get the correct subtitle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(laughs)'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subs.at(flip_times[0]).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's my boy.\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subs.at(add_time_offset(flip_times[0], 500)).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using list comprehension, we add an offset of .8 seconds to all the flip times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.time(0, 12, 48, 100000),\n",
       " datetime.time(0, 12, 49, 700000),\n",
       " datetime.time(0, 12, 51, 200000),\n",
       " datetime.time(0, 13, 1, 600000),\n",
       " datetime.time(0, 13, 6, 900000),\n",
       " datetime.time(0, 13, 12, 100000),\n",
       " datetime.time(0, 13, 14, 200000),\n",
       " datetime.time(0, 13, 16, 400000),\n",
       " datetime.time(0, 13, 17, 100000),\n",
       " datetime.time(0, 13, 17, 800000),\n",
       " datetime.time(0, 13, 28, 800000),\n",
       " datetime.time(0, 13, 29, 500000)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offset_flip_times = [add_time_offset(flip_time, offset=800) for flip_time in flip_times]\n",
    "offset_flip_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's my boy.\n",
      "\n",
      "No wonder we left\n",
      "the family business.\n",
      "\n",
      "She loves you, you know.\n",
      "\n",
      "Used to.\n",
      "\n",
      "Well, one day, I just hope\n",
      "that I walk through that door\n",
      "\n",
      "How many years you got left?\n",
      "\n",
      "Two, with good behavior.\n",
      "\n",
      "-So how many, really?\n",
      "-Four.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# these are all correct\n",
    "for time in offset_flip_times[0:8]:\n",
    "    print(subs.at(time).text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtitle Attribution\n",
    "We've gotten the correct subtitles for all flip times. Next, we just have to collect all the subtitles, including the ones between these flip times. Here are the indices where we've identified the flips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[265, 266, 267, 271, 272, 274, 275, 276]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtitle_flip_indices = []\n",
    "for time in offset_flip_times[0:8]:\n",
    "    subtitle_flip_indices.append(subs.at(time)[0].index)\n",
    "subtitle_flip_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, `subs[265]` belongs to character T, `subs[266]` belongs to character S, and `subs[267]` through `subs[270]` belongs to character T. We build a list for each character containing the index for each of their subtitle lines. This will be be defined as a function in `dialogue_attribution_io.py` called `attribute_subs()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[266, 271, 274]\n",
      "[265, 267, 268, 269, 270, 272, 273, 275]\n"
     ]
    }
   ],
   "source": [
    "character_s_subs = []\n",
    "character_t_subs = []\n",
    "character = 1 # since we're just working with two-character scenes for now, this value flips between 1 and -1\n",
    "sub_index_start = subtitle_flip_indices[0]\n",
    "sub_index_finish = subtitle_flip_indices[-1] # this discards final subtitles, for now\n",
    "\n",
    "for index in range(sub_index_start, sub_index_finish):\n",
    "    if index in subtitle_flip_indices:\n",
    "        character *= -1\n",
    "    if character == 1:\n",
    "        character_s_subs.append(index)\n",
    "    if character == -1:\n",
    "        character_t_subs.append(index)    \n",
    "    \n",
    "print(character_s_subs)\n",
    "print(character_t_subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['No wonder we left\\nthe family business.',\n",
       " 'Used to.',\n",
       " 'How many years you got left?']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_s_lines = []\n",
    "for index in character_s_subs:\n",
    "    character_s_lines.append(subs[index].text)\n",
    "character_s_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"That's my boy.\",\n",
       " 'She loves you, you know.',\n",
       " 'All you got to do is\\npick up the phone, Decks.',\n",
       " \"Look at me.\\nYou're her big brother.\",\n",
       " 'She looks up to you.',\n",
       " 'Well, one day, I just hope\\nthat I walk through that door',\n",
       " 'and I see the two of you\\nsitting there.',\n",
       " 'Two, with good behavior.']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_t_lines = []\n",
    "for index in character_t_subs:\n",
    "    character_t_lines.append(subs[index].text)\n",
    "character_t_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us lists for each character, each containing the subtitle lines for each characters. Development on this module will pause for now, since there's some difficulty in merging these lists with the one-second-per-frame DataFrame. That DataFrame and the subtitles operate on a different time system, and the time offset introduces another headache."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (moviegoer)",
   "language": "python",
   "name": "moviegoer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
