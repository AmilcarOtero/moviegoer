{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# self_intro_character_identification\n",
    "Since we've explored the visual, audio, and subtitle tracks and extracted features from each, we can start to use them all together to accomplish broader *Moviegoer* goals. This notebook is the first example of this.\n",
    "\n",
    "We'll be generating a list of possible characters, then looking for these names in self-identifications (\"My name is Alice.\" or \"I'm Ben.\"). Then we can build a composite, \"average\" encoding of their face, so we can track them throughout the film, every time we spot their face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../subtitle_features')\n",
    "from subtitle_dataframes_io import *\n",
    "from subtitle_auxiliary_io import *\n",
    "sys.path.append('../vision_features')\n",
    "from vision_dataframes_io import *\n",
    "sys.path.append('../audio_features')\n",
    "from audio_dataframes_io import *\n",
    "from time_reference_io import *\n",
    "import datetime\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the Film's Character List\n",
    "We'll generate the `subtitle_df` and `sentence_df` dataframes based off the subtitle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = pysrt.open('../subtitles/plus_one.srt')\n",
    "subtitle_df = generate_base_subtitle_df(subs)\n",
    "subtitle_df = generate_subtitle_features(subtitle_df)\n",
    "subtitle_df['cleaned_text'] = subtitle_df['concat_sep_text'].map(clean_line)\n",
    "sentences = partition_sentences(remove_blanks(subtitle_df['cleaned_text'].tolist()), nlp)\n",
    "subtitle_indices = tie_sentence_subtitle_indices(sentences, subtitle_df)\n",
    "sentence_df = pd.DataFrame(list(zip(sentences, subtitle_indices)), columns=['sentence', 'subtitle_indices'])\n",
    "sentence_df = generate_sentence_features(sentence_df, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've previously defined two functions that will read through the subtitles and count up character names, either mentioned as dialogue, or labelling an offscreen speaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_sub_mentions = character_subtitle_mentions(sentences, nlp)\n",
    "chars_sub_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chars_offscreen_speakers = character_offscreen_speakers(subtitle_df)\n",
    "chars_offscreen_speakers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take the most common names, and assume they're the main characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = []\n",
    "\n",
    "for character in chars_sub_mentions:\n",
    "    if character[1] >= 10:\n",
    "        characters.append(character[0].lower())\n",
    "\n",
    "for character in chars_offscreen_speakers:\n",
    "    if character[1] >= 5: \n",
    "        characters.append(character[0].lower())\n",
    "        \n",
    "characters = list(set(characters))\n",
    "characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Self-Introduction Sentences\n",
    "For this exercise, we're going to identify characters solely based on self-introductions. We have a function to find phrases like \"My name is Alice.\" or \"I'm Ben.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence_df[sentence_df['self_intro'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we'll focus on Ben's self-introductions. The film has five sentences where he introduces himself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ben_string = 'ben'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_df[sentence_df.self_intro.str.contains(ben_string, na=False, case=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the Film Times and Frames\n",
    "From the `sentence_df`, we need to look for Ben's self-introductions and find the indices in `subtitle_df`, which contains times of the actual subtitles (not sentences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ben_indices = sentence_df[sentence_df.self_intro.str.contains(ben_string, na=False, case=False)].subtitle_indices.values\n",
    "ben_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ben_flattened_indices = np.concatenate(ben_indices).ravel()\n",
    "ben_flattened_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that one of these sentences spans two separate subtitles. We'll leave both of these in, because if it spans such a long duration, there's a good chance it'll have Ben's face onscreen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtitle_df[subtitle_df.index.isin(ben_flattened_indices)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each subtitle, we'll calculate the `mid_time`, or the difference between the start and end times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_time = subtitle_mid_time(subtitle_df.iloc[506].start_time, subtitle_df.iloc[506].end_time)\n",
    "time_to_frame(mid_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_time_frames = []\n",
    "for sub_index in ben_flattened_indices:\n",
    "    mid_time = subtitle_mid_time(subtitle_df.iloc[sub_index].start_time, subtitle_df.iloc[sub_index].end_time)\n",
    "    mid_time_frames.append(time_to_frame(mid_time))\n",
    "\n",
    "mid_time_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Face Encodings\n",
    "We can now search all the frames (images) we gathered and collect the face encodings. From six frames, it looks like we were only able to find five encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_choice = 'plus_one'\n",
    "\n",
    "ben_encodings = []\n",
    "\n",
    "for frame_number in mid_time_frames:\n",
    "    frame = load_frame(movie_choice, frame_number)\n",
    "\n",
    "    locations = face_recognition.face_locations(frame, number_of_times_to_upsample=2)\n",
    "    encodings = face_recognition.face_encodings(frame, locations)\n",
    "    if encodings:\n",
    "        ben_encodings.append(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(ben_encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Face Encodings\n",
    "We can now use the `compare_faces()` function to compare the five encodings to each other. The first two faces appear to not match any other faces, while the final three all match each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ben_scratch = ben_encodings.copy()\n",
    "ben_flattened = []\n",
    "for x in ben_scratch:\n",
    "    for y in x:\n",
    "        ben_flattened.append(y)\n",
    "ben_compare = ben_flattened[0]\n",
    "del ben_flattened[0]\n",
    "face_recognition.compare_faces(ben_flattened, ben_compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ben_scratch = ben_encodings.copy()\n",
    "ben_flattened = []\n",
    "for x in ben_scratch:\n",
    "    for y in x:\n",
    "        ben_flattened.append(y)\n",
    "ben_compare = ben_flattened[1]\n",
    "del ben_flattened[1]\n",
    "face_recognition.compare_faces(ben_flattened, ben_compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ben_scratch = ben_encodings.copy()\n",
    "ben_flattened = []\n",
    "for x in ben_scratch:\n",
    "    for y in x:\n",
    "        ben_flattened.append(y)\n",
    "ben_compare = ben_flattened[2]\n",
    "del ben_flattened[2]\n",
    "face_recognition.compare_faces(ben_flattened, ben_compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ben_scratch = ben_encodings.copy()\n",
    "ben_flattened = []\n",
    "for x in ben_scratch:\n",
    "    for y in x:\n",
    "        ben_flattened.append(y)\n",
    "ben_compare = ben_flattened[3]\n",
    "del ben_flattened[3]\n",
    "face_recognition.compare_faces(ben_flattened, ben_compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ben_scratch = ben_encodings.copy()\n",
    "ben_flattened = []\n",
    "for x in ben_scratch:\n",
    "    for y in x:\n",
    "        ben_flattened.append(y)\n",
    "ben_compare = ben_flattened[4]\n",
    "del ben_flattened[4]\n",
    "face_recognition.compare_faces(ben_flattened, ben_compare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we looked at five possible Ben face encodings, and found that three of them match each other. We can assume that this is Ben's face.\n",
    "\n",
    "Finding a majority of faces that match each other is a good threshold. We can automate the above process to create an array of the three encodings that match one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ben_scratch = ben_encodings.copy()\n",
    "ben_flattened = []\n",
    "for x in ben_scratch:\n",
    "    for y in x:\n",
    "        ben_flattened.append(y)\n",
    "\n",
    "good_bens = []\n",
    "x = 0\n",
    "face_candidates = len(ben_flattened)\n",
    "\n",
    "while x < face_candidates:\n",
    "    ben_loop = ben_flattened.copy()\n",
    "    ben_compare = ben_loop[x]\n",
    "    del ben_loop[x]\n",
    "    if sum(face_recognition.compare_faces(ben_loop, ben_compare)) >= (face_candidates - 1)/2:\n",
    "        good_bens.append(ben_compare)\n",
    "    x += 1\n",
    "\n",
    "good_bens = np.array(good_bens)\n",
    "\n",
    "len(good_bens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (moviegoer)",
   "language": "python",
   "name": "moviegoer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
