{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scene_clustering\n",
    "This notebook contains inital code for clustering frames into shots, identifying the A/B/A/B pattern, and using the image classifier model to see if they're MCUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('site-packages') # manually put all packages/libraries into this folder\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras import models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "### For POC, designating a specific scene's worth of frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose film and frames\n",
    "film = 'hustle'\n",
    "frame_choice = list(range(600, 1000)) # The Hustle, threshold 3100, +/- ~100 frames either side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5877 images in the folder\n",
      "Selected 400 of those frames\n"
     ]
    }
   ],
   "source": [
    "# establish folder for this film\n",
    "dialogue_folder = os.path.join('dialogue_frames', film)\n",
    "\n",
    "print('There are', len(os.listdir(dialogue_folder)), 'images in the folder')\n",
    "print('Selected', len(frame_choice), 'of those frames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = VGG16(weights='imagenet', include_top=False)\n",
    "model.summary()\n",
    "\n",
    "vgg16_feature_list = []\n",
    "\n",
    "\n",
    "for x in frame_choice:\n",
    "    img_path = dialogue_folder + '/' + film + '_frame'+ str(x) + '.jpg'\n",
    "    img = image.load_img(img_path, target_size=(256, 256))\n",
    "    img_data = image.img_to_array(img)\n",
    "    img_data = np.expand_dims(img_data, axis=0)\n",
    "    img_data = preprocess_input(img_data)\n",
    "\n",
    "    vgg16_feature = model.predict(img_data)\n",
    "    vgg16_feature_np = np.array(vgg16_feature)\n",
    "    vgg16_feature_list.append(vgg16_feature_np.flatten())\n",
    "\n",
    "    x += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 32768)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to NumPy array and verify shape\n",
    "vgg16_feature_list_np = np.array(vgg16_feature_list)\n",
    "vgg16_feature_list_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 37\n",
      "[12 12 29 29 29 23 23 23 23 23 23 23 23 23 23 12 12 12 12 12 12 12  2  2\n",
      "  2  2 20 20 20 20  8  8  8  8  8  8  8  8  8 11 11 11 28 28 28 11 11 11\n",
      "  5  5  5  5  5 25 25 17 17 31 31 31 31 31 31  1  1  1 14 14 14 14  1  1\n",
      "  1  1  1 35 31 31 31 31 14 14  5  5  5 17 17 17 17 10 10 10  4  4 30 30\n",
      " 30 30 10 10  4  4 17 17 17 25 25 25 25  5  5  5 35 17 17 17 17 17 17 17\n",
      " 35 35 35 35 30 30  4  4  4  4 30 30 30 30  4  4  1  1  1  1  1 14 14 14\n",
      " 14 14 30 30 30 35 35 35 35 30 30 30 35 30 10 10 10 26 26 26 27 27 27 27\n",
      " 27 22 22 22 22 22 22 22 22  8  8  8  8  8 27 27 27 27 27 27 27 27  2  2\n",
      "  2  2 27  2  2  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  9  9\n",
      "  9  9  9  9 33 33 33  9  9 33  9  9  9  9  9  9  9  9 33 33 33 33 33  9\n",
      "  9  9  9  9  9  9  9  9 21 21 21 21 21 21 21 21 21 21 21 33 33 33 33 33\n",
      "  0  0  0 33 33  9  9 33 33 33  9  9  9  9  9  0  0  0  0  0  2  2  2 33\n",
      " 33 33  0  0  0  0  0  0  9  9  9  9 33 33  9  9  9  9 33 33 33 33  9  9\n",
      "  9  9  9  9  9  9 19 19 19 19 19 19 19 19 19  2  2  2 26 26 26 26 18 18\n",
      " 18 18  2  2  2  2 13 13 13 13  2  2  2  2 34 34 34  2  2  2  2  2  1  1\n",
      "  1  1  1  1  1 32 32 32 32 32 36 36 36  6  6  6  6  6  6  7  7  7 16 16\n",
      " 16 16 16 16 24 24 24 24 15 15 15 15 15 15 15  2]\n"
     ]
    }
   ],
   "source": [
    "hac = AgglomerativeClustering(n_clusters = None, distance_threshold = 3000).fit(vgg16_feature_list_np)\n",
    "hac_labels = hac.labels_\n",
    "print('Number of clusters:', hac.n_clusters_)\n",
    "print(hac_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Saved Model and Identify MCUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model = models.load_model('saved_models/tuned_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = []\n",
    "for x in frame_choice:\n",
    "    image_list.append(img_to_array(load_img(dialogue_folder + '/' + film + '_frame'+ str(x) + '.jpg', target_size = (128, 128), color_mode = 'grayscale')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_array = np.array(image_list)\n",
    "y_pred = tuned_model.predict_classes(image_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model's predict_classes method creates a NumPy array of arrays; this converts it to a list of 0/1 integers\n",
    "y_pred_values = []\n",
    "for prediction in y_pred:\n",
    "    y_pred_values.append(prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shot_ids(frame_choice, hac_labels):\n",
    "    shot_id = 0\n",
    "    shot_id_list = []\n",
    "    prev_frame = 1000\n",
    "\n",
    "    for frame_file, cluster in zip(frame_choice, hac_labels):\n",
    "        if cluster != prev_frame and prev_frame != 1000:\n",
    "            shot_id += 1\n",
    "        shot_id_list.append(shot_id)\n",
    "        # print(frame_file, '\\t', mcu_flag, '\\t', cluster, '\\t', prev_frame, '\\t', shot_id,'\\tend')\n",
    "        prev_frame = cluster\n",
    "    \n",
    "    return shot_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "shot_id_list = get_shot_ids(frame_choice, hac_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame_file</th>\n",
       "      <th>cluster</th>\n",
       "      <th>shot_id</th>\n",
       "      <th>mcu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>600</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>601</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>602</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>603</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>604</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>605</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>606</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   frame_file  cluster  shot_id  mcu\n",
       "0         600       12        0    0\n",
       "1         601       12        0    0\n",
       "2         602       29        1    0\n",
       "3         603       29        1    0\n",
       "4         604       29        1    0\n",
       "5         605       23        2    0\n",
       "6         606       23        2    0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scene_df = pd.DataFrame(zip(frame_choice, hac_labels, shot_id_list, y_pred_values), columns=['frame_file', 'cluster', 'shot_id', 'mcu'])\n",
    "scene_df.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scene Pattern Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if ABAB, check for MCU, then establish as scene\n",
    "# look for first A/B, last A/B, and get all Cs in between\n",
    "# scene is first A/B, last A/B, and any connecting Cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speaker_pairs(frame_choice, hac_labels, y_pred_values, shot_id_list):\n",
    "    \n",
    "    prev_clust_1 = 1001\n",
    "    prev_clust_1_list = []\n",
    "    prev_clust_2 = 1002\n",
    "    prev_clust_2_list = []\n",
    "    prev_clust_3 = 1003\n",
    "    prev_clust_3_list = []\n",
    "    prev_shot_id = -1\n",
    "    speaker_a_list = []\n",
    "    speaker_b_list = []\n",
    "\n",
    "    for frame_file, cluster, mcu_flag, shot_id in zip(frame_choice, hac_labels, y_pred_values, shot_id_list):\n",
    "        # print(frame_file, '\\t', mcu_flag, '\\t', cluster,'\\t', shot_id, '\\t', prev_shot_id, '\\t', prev_clust_1, '\\t', prev_clust_2, '\\t', prev_clust_3, '\\tbeg')\n",
    "        if cluster == prev_clust_2 and prev_clust_1 == prev_clust_3:\n",
    "            speaker_a_list.append(min(cluster, prev_clust_1))\n",
    "            speaker_b_list.append(max(cluster, prev_clust_1))\n",
    "\n",
    "        if shot_id != prev_shot_id:\n",
    "            prev_shot_id = shot_id\n",
    "            prev_clust_3 = prev_clust_2\n",
    "            prev_clust_2 = prev_clust_1\n",
    "            prev_clust_1 = cluster\n",
    "        prev_clust_1_list.append(prev_clust_1)\n",
    "        prev_clust_2_list.append(prev_clust_2)\n",
    "        prev_clust_3_list.append(prev_clust_3)\n",
    "        # print(frame_file, '\\t', mcu_flag, '\\t', cluster,'\\t', shot_id, '\\t', prev_shot_id, '\\t', prev_clust_1, '\\t', prev_clust_2, '\\t', prev_clust_3, '\\tend')\n",
    "    \n",
    "    speaker_pairs = []\n",
    "    for a, b, in zip(speaker_a_list, speaker_b_list):\n",
    "        if [int(a), int(b)] not in speaker_pairs:\n",
    "            speaker_pairs.append([int(a), int(b)])\n",
    "    \n",
    "    return speaker_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 30], [30, 35], [2, 27], [9, 33]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_pairs = get_speaker_pairs(frame_choice, hac_labels, y_pred_values, shot_id_list)\n",
    "speaker_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster\t count\t mcu probability\n",
      "4 \t 10 \t 0.00%\n",
      "30 \t 17 \t 88.24%\n",
      "\n",
      "30 \t 17 \t 88.24%\n",
      "35 \t 11 \t 81.82%\n",
      "\n",
      "2 \t 30 \t 33.33%\n",
      "27 \t 14 \t 100.00%\n",
      "\n",
      "9 \t 48 \t 97.92%\n",
      "33 \t 28 \t 100.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('cluster\\t', 'count\\t', 'mcu probability')\n",
    "for pair in speaker_pairs:\n",
    "    print(pair[0], '\\t', scene_df.loc[scene_df['cluster'] == pair[0]]['mcu'].count(), '\\t', '{0:.2f}%'.format(scene_df.loc[scene_df['cluster'] == pair[0]]['mcu'].mean() * 100))\n",
    "    print(pair[1], '\\t', scene_df.loc[scene_df['cluster'] == pair[1]]['mcu'].count(), '\\t', '{0:.2f}%'.format(scene_df.loc[scene_df['cluster'] == pair[1]]['mcu'].mean() * 100))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scenes(speaker_pairs, scene_df):\n",
    "    scenes = []\n",
    "\n",
    "    for pair in speaker_pairs:\n",
    "        mean_a = scene_df.loc[scene_df['cluster'] == pair[0]]['mcu'].mean()\n",
    "        mean_b = scene_df.loc[scene_df['cluster'] == pair[1]]['mcu'].mean()\n",
    "        if mean_a > .5 and mean_b > .5:\n",
    "            dialogue_start = scene_df.loc[(scene_df['cluster'] == pair[0]) | (scene_df['cluster'] == pair[1])].frame_file.min()\n",
    "            dialogue_end = scene_df.loc[(scene_df['cluster'] == pair[0]) | (scene_df['cluster'] == pair[1])].frame_file.max()\n",
    "            cutaways = scene_df.loc[(scene_df['frame_file'] > dialogue_start) & (scene_df['frame_file'] < dialogue_end)].cluster.unique()\n",
    "            cutaways = cutaways[cutaways != pair[0]]\n",
    "            cutaways = cutaways[cutaways != pair[1]]\n",
    "            print('Speaker A and B clusters:', pair)\n",
    "            print('First, last frames of speakers A and B:', dialogue_start, dialogue_end)\n",
    "            print('Cutaway clusters:', cutaways)\n",
    "\n",
    "            min_flag = 0\n",
    "            while min_flag == 0:\n",
    "                try:\n",
    "                    if int(scene_df.loc[scene_df['frame_file'] == (dialogue_start - 1)].cluster) in cutaways:\n",
    "                        dialogue_start -= 1\n",
    "                    else:\n",
    "                        min_flag = 1\n",
    "                except TypeError: # error if hitting the beginning of the frame list\n",
    "                    min_flag = 1\n",
    "\n",
    "            max_flag = 0\n",
    "            while max_flag == 0:\n",
    "                try:\n",
    "                    if int(scene_df.loc[scene_df['frame_file'] == (dialogue_end + 1)].cluster) in cutaways:\n",
    "                        dialogue_end += 1\n",
    "                    else:\n",
    "                        max_flag = 1\n",
    "                except TypeError: # error if hitting the end of the frame list\n",
    "                    max_flag = 1\n",
    "\n",
    "            print('First, last frames of entire scene:', dialogue_start, dialogue_end)\n",
    "\n",
    "            scenes.append((dialogue_start, dialogue_end))\n",
    "            \n",
    "    return scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker A and B clusters: [30, 35]\n",
      "First, last frames of speakers A and B: 675 757\n",
      "Cutaway clusters: [31 14  5 17 10  4 25  1]\n",
      "First, last frames of entire scene: 648 760\n",
      "Speaker A and B clusters: [9, 33]\n",
      "First, last frames of speakers A and B: 814 917\n",
      "Cutaway clusters: [21  0  2]\n",
      "First, last frames of entire scene: 814 917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(648, 760), (814, 917)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenes = get_scenes(speaker_pairs, scene_df)\n",
    "scenes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
