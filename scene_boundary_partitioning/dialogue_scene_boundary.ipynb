{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dialogue_scene_boundary\n",
    "This notebook contains preliminary code for identifying two-character dialogue scenes' beginnings and ends, based on common scene layouts found in film editing. We'll also be using a CNN image model trained elsewhere in this repository, to determine if shots are Medium Close-Ups, the most common cinematography shot used for two-character dialogue scenes.\n",
    "\n",
    "As an example, we'll be analyzing 400 frames from the film \"The Hustle\". These frames, each representing one second of the film, depict two consecutive full scenes and portions of the two scenes before and after. We'll be trying to identify the beginning and end frames for each of the two full scenes.\n",
    "\n",
    "It's *strongly* recommended to follow along using the readme. It's very helpful to see the frames represented as actual images, instead of just abstract file or cluster numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras import models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "## Designating film and frames\n",
    "We select frames 600-999 from The Hustle's directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose film and frames\n",
    "film = 'hustle'\n",
    "frame_choice = list(range(600, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5877 images in the folder\n",
      "Selected 400 of those frames\n"
     ]
    }
   ],
   "source": [
    "# establish folder for this film\n",
    "dialogue_folder = os.path.join('dialogue_frames', film)\n",
    "\n",
    "print('There are', len(os.listdir(dialogue_folder)), 'images in the folder')\n",
    "print('Selected', len(frame_choice), 'of those frames')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16 Vectorization\n",
    "Keras' VGG16 CNN model will be used to vectorize the input frames. We'll be using the \"imagenet\" weights, the result of VGG16's training on 15 million images in the imagenet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 62s 1us/step\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = VGG16(weights='imagenet', include_top=False)\n",
    "model.summary()\n",
    "\n",
    "vgg16_feature_list = []\n",
    "\n",
    "\n",
    "for x in frame_choice:\n",
    "    img_path = dialogue_folder + '/' + film + '_frame'+ str(x) + '.jpg'\n",
    "    img = image.load_img(img_path, target_size=(256, 256))\n",
    "    img_data = image.img_to_array(img)\n",
    "    img_data = np.expand_dims(img_data, axis=0)\n",
    "    img_data = preprocess_input(img_data)\n",
    "\n",
    "    vgg16_feature = model.predict(img_data)\n",
    "    vgg16_feature_np = np.array(vgg16_feature)\n",
    "    vgg16_feature_list.append(vgg16_feature_np.flatten())\n",
    "\n",
    "    x += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 32768)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to NumPy array and verify shape\n",
    "vgg16_feature_list_np = np.array(vgg16_feature_list)\n",
    "vgg16_feature_list_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "A HAC clustering object will be fit to the VGG16's vectorization of input frames. We set the distance_threshold to 3000: a higher threshold means fewer clusters, and vice versa. This threshold can be tuned for better results during future development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 37\n",
      "[12 12 29 29 29 23 23 23 23 23 23 23 23 23 23 12 12 12 12 12 12 12  2  2\n",
      "  2  2 20 20 20 20  8  8  8  8  8  8  8  8  8 11 11 11 28 28 28 11 11 11\n",
      "  5  5  5  5  5 25 25 17 17 31 31 31 31 31 31  1  1  1 14 14 14 14  1  1\n",
      "  1  1  1 35 31 31 31 31 14 14  5  5  5 17 17 17 17 10 10 10  4  4 30 30\n",
      " 30 30 10 10  4  4 17 17 17 25 25 25 25  5  5  5 35 17 17 17 17 17 17 17\n",
      " 35 35 35 35 30 30  4  4  4  4 30 30 30 30  4  4  1  1  1  1  1 14 14 14\n",
      " 14 14 30 30 30 35 35 35 35 30 30 30 35 30 10 10 10 26 26 26 27 27 27 27\n",
      " 27 22 22 22 22 22 22 22 22  8  8  8  8  8 27 27 27 27 27 27 27 27  2  2\n",
      "  2  2 27  2  2  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  9  9\n",
      "  9  9  9  9 33 33 33  9  9 33  9  9  9  9  9  9  9  9 33 33 33 33 33  9\n",
      "  9  9  9  9  9  9  9  9 21 21 21 21 21 21 21 21 21 21 21 33 33 33 33 33\n",
      "  0  0  0 33 33  9  9 33 33 33  9  9  9  9  9  0  0  0  0  0  2  2  2 33\n",
      " 33 33  0  0  0  0  0  0  9  9  9  9 33 33  9  9  9  9 33 33 33 33  9  9\n",
      "  9  9  9  9  9  9 19 19 19 19 19 19 19 19 19  2  2  2 26 26 26 26 18 18\n",
      " 18 18  2  2  2  2 13 13 13 13  2  2  2  2 34 34 34  2  2  2  2  2  1  1\n",
      "  1  1  1  1  1 32 32 32 32 32 36 36 36  6  6  6  6  6  6  7  7  7 16 16\n",
      " 16 16 16 16 24 24 24 24 15 15 15 15 15 15 15  2]\n"
     ]
    }
   ],
   "source": [
    "hac = AgglomerativeClustering(n_clusters = None, distance_threshold = 3000).fit(vgg16_feature_list_np)\n",
    "hac_labels = hac.labels_\n",
    "print('Number of clusters:', hac.n_clusters_)\n",
    "print(hac_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Saved Model and Generate MCU Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've previously trained a CNN model to identify if a movie frame is a Medium Close-Up or not. MCUs are the most common cinematography shot of dialogue scenes. We'll use the model to make predictions on each frame and use them alter in the scene identification process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model = models.load_model('saved_models/tuned_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = []\n",
    "for x in frame_choice:\n",
    "    image_list.append(img_to_array(load_img(dialogue_folder + '/' + film + '_frame'+ str(x) + '.jpg', target_size = (128, 128), color_mode = 'grayscale')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_array = np.array(image_list)\n",
    "y_pred = tuned_model.predict_classes(image_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model's predict_classes method creates a NumPy array of arrays; this converts it to a list of 0/1 integers\n",
    "y_pred_values = []\n",
    "for prediction in y_pred:\n",
    "    y_pred_values.append(prediction[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we create the DataFrame, we'll create an identification system for an individual shot. Each time the cluster value changes, it's a new shot. Multiple shots can share the same cluster value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "shot_id = 0\n",
    "shot_id_list = []\n",
    "prev_frame = 1000\n",
    "\n",
    "for frame_file, cluster in zip(frame_choice, hac_labels):\n",
    "    if cluster != prev_frame and prev_frame != 1000:\n",
    "        shot_id += 1\n",
    "    shot_id_list.append(shot_id)\n",
    "    prev_frame = cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame is created using the frame_file, its cluster, its shot_id, and its MCU prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame_file</th>\n",
       "      <th>cluster</th>\n",
       "      <th>shot_id</th>\n",
       "      <th>mcu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>600</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>601</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>602</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>603</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>604</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>605</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>606</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   frame_file  cluster  shot_id  mcu\n",
       "0         600       12        0    0\n",
       "1         601       12        0    0\n",
       "2         602       29        1    0\n",
       "3         603       29        1    0\n",
       "4         604       29        1    0\n",
       "5         605       23        2    0\n",
       "6         606       23        2    0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scene_df = pd.DataFrame(zip(frame_choice, hac_labels, shot_id_list, y_pred_values), columns=['frame_file', 'cluster', 'shot_id', 'mcu'])\n",
    "scene_df.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scene Identification\n",
    "*Note: it is strongly recommended to follow along using the readme; the example code can be better understood by seeing the actual frames in the readme.*\n",
    "\n",
    "Two-character dialogue scenes are primarily comprised of two speakers, each speaking in an alternating pattern: Speaker A, Speaker B, Speaker A, Speaker B, etc. We'll be looking for clusters that fit this pattern, and verify that they are Medium Close-Ups, the standard cinemtography shot for dialogue scenes, using the MCU image classifier model.\n",
    "\n",
    "Clusters representing Speaker A and Speaker B are the Anchor clusters, and a rough designation of the scene can be defined by the first and last frames with Anchor clusters.\n",
    "\n",
    "The scene can be further expanded by considering the Cutaway clusters, any cluster that appears within the Anchor scene boundary. If these appear shortly before the Anchor start or after the Anchor end, they can be considered part of the scene. \n",
    "\n",
    "1. Check all clusters for each pair of two clusters that form an A/B/A/B pattern\n",
    "2. Verify that each of the two clusters in each pattern are Medium Close-Ups (MCUs), and discard patterns containing non-MCUs\n",
    "3. Identify the earliest and latest frames with either speaker cluster, to determine the scene's Anchor start and end\n",
    "4. Identify all clusters that lie in between the Anchor start and end frames, to determine the Cutaway clusters\n",
    "5. Expand the scene in either direction by checking for adjacent Cutaway clusters before the starting Anchor and after the ending Anchor\n",
    "\n",
    "Below is a step-by-step walkthrough, followed by the same code bundled into functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Identifying A/B/A/B cluster pairs\n",
    "The first step is finding cluster pairs that form an A/B/A/B pattern. Every time there's a new shot (cluster), we store the previous cluster in memory; we'll need to do this for the previous three clusters. When the current cluster matches prev_clust_2, and when prev_clust_1 matches prev_clust_3, we have an A/B/A/B pattern.\n",
    "\n",
    "The below dataframe shows an example of an A/B/A/B cluster pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame_file</th>\n",
       "      <th>cluster</th>\n",
       "      <th>shot_id</th>\n",
       "      <th>mcu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>747</td>\n",
       "      <td>30</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>748</td>\n",
       "      <td>30</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>749</td>\n",
       "      <td>35</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>750</td>\n",
       "      <td>35</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>751</td>\n",
       "      <td>35</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>752</td>\n",
       "      <td>35</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>753</td>\n",
       "      <td>30</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>754</td>\n",
       "      <td>30</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>755</td>\n",
       "      <td>30</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>756</td>\n",
       "      <td>35</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     frame_file  cluster  shot_id  mcu\n",
       "147         747       30       39    1\n",
       "148         748       30       39    1\n",
       "149         749       35       40    1\n",
       "150         750       35       40    0\n",
       "151         751       35       40    1\n",
       "152         752       35       40    1\n",
       "153         753       30       41    1\n",
       "154         754       30       41    1\n",
       "155         755       30       41    1\n",
       "156         756       35       42    1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scene_df.loc[(scene_df['frame_file'] > 746) & (scene_df['frame_file'] < 757)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 30], [30, 35], [2, 27], [9, 33]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check for an A/B/A/B pattern, we must store the previous three clusters in memory\n",
    "prev_clust_1 = 1001\n",
    "prev_clust_2 = 1002\n",
    "prev_clust_3 = 1003\n",
    "prev_shot_id = -1\n",
    "alternate_a_list = []\n",
    "alternate_b_list = []\n",
    "\n",
    "# zip our various lists into a usable data structure\n",
    "for frame_file, cluster, mcu_flag, shot_id in zip(frame_choice, hac_labels, y_pred_values, shot_id_list):\n",
    "    # when iterating through each frame, look for an A/B/A/B pattern, and save the clusters of any patterns\n",
    "    if cluster == prev_clust_2 and prev_clust_1 == prev_clust_3:\n",
    "        alternate_a_list.append(min(cluster, prev_clust_1)) # min and max are used to avoid duplicates of (1, 2), (2, 1)\n",
    "        alternate_b_list.append(max(cluster, prev_clust_1))\n",
    "\n",
    "    # we use prev_shot_id to identify when there's a new shot (when the cluster value changes)\n",
    "    # every time there's a new shot, we update the cluster memory\n",
    "    if shot_id != prev_shot_id:\n",
    "        prev_shot_id = shot_id\n",
    "        prev_clust_3 = prev_clust_2\n",
    "        prev_clust_2 = prev_clust_1\n",
    "        prev_clust_1 = cluster\n",
    "        \n",
    "    # the below print can be used for troubleshooting and visualizing the memory state at each frame\n",
    "    # print(frame_file, '\\t', mcu_flag, '\\t', cluster,'\\t', shot_id, '\\t', prev_shot_id, '\\t', prev_clust_1, '\\t', prev_clust_2, '\\t', prev_clust_3, '\\tend')\n",
    "\n",
    "# save unique alternating pairs\n",
    "alternating_pairs = []\n",
    "for a, b, in zip(alternate_a_list, alternate_b_list):\n",
    "    if [int(a), int(b)] not in alternating_pairs:\n",
    "        alternating_pairs.append([int(a), int(b)])\n",
    "        \n",
    "alternating_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Checking if both clusters are MCUs\n",
    "Although we now have cluster pairs that form an A/B/A/B pattern, we're not sure if these actually represent characters. We'll check each pair to determine if both clusters represent Medium Close-Ups, the classic cinematography shot for dialogue scenes. We evaluate the predictions for EVERY frame assigned to a specific cluster.\n",
    "\n",
    "This is necessary because the MCU-identification model is more discriminating than the image clustering algorithm (at its current threshold). So a few frames might be grouped in the same cluster but still have differing MCU/non-MCU predictions.\n",
    "\n",
    "For now, we'll only accept cluster pairs if both clusters have a MCU-prediction mean greater than .5. We discard two pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster\t count\t mcu probability\n",
      "4 \t 10 \t 0.00%\n",
      "30 \t 17 \t 88.24%\n",
      "Fails MCU check\n",
      "\n",
      "30 \t 17 \t 88.24%\n",
      "35 \t 11 \t 81.82%\n",
      "Passes MCU check\n",
      "\n",
      "2 \t 30 \t 33.33%\n",
      "27 \t 14 \t 100.00%\n",
      "Fails MCU check\n",
      "\n",
      "9 \t 48 \t 97.92%\n",
      "33 \t 28 \t 100.00%\n",
      "Passes MCU check\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[30, 35], [9, 33]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_pairs = []\n",
    "print('cluster\\t', 'count\\t', 'mcu probability')\n",
    "\n",
    "for pair in alternating_pairs:\n",
    "    # calculate the mean of each cluster's MCU column\n",
    "    mean_a = scene_df.loc[scene_df['cluster'] == pair[0]]['mcu'].mean()\n",
    "    mean_b = scene_df.loc[scene_df['cluster'] == pair[1]]['mcu'].mean()\n",
    "    print(pair[0], '\\t', scene_df.loc[scene_df['cluster'] == pair[0]]['mcu'].count(), '\\t', '{0:.2f}%'.format(mean_a * 100))\n",
    "    print(pair[1], '\\t', scene_df.loc[scene_df['cluster'] == pair[1]]['mcu'].count(), '\\t', '{0:.2f}%'.format(mean_b * 100))\n",
    "    \n",
    "    # an alternating pair will pass the MCU check if BOTH clusters have a MCU mean greater than .5\n",
    "    if mean_a > .5 and mean_b > .5:\n",
    "        print('Passes MCU check')\n",
    "        speaker_pairs.append(pair)\n",
    "    else:\n",
    "        print('Fails MCU check')\n",
    "    print()\n",
    "    \n",
    "speaker_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Establishing first and last frames of Anchor clusters\n",
    "After checking that the cluster pairs are indeed MCUs, we can assume that they represent shots of Speakers A and B. As a preliminary designation of a given scene, we can designate the earliest frame and last frame containing EITHER of these shots as the anchor_start and anchor_end, a primitive definition of the scene's start and end frames.\n",
    "\n",
    "Since we have two speaker_pairs that passed the MCU check, we'll pick one as an example to continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30, 35]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair = speaker_pairs[0]\n",
    "pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame_file</th>\n",
       "      <th>cluster</th>\n",
       "      <th>shot_id</th>\n",
       "      <th>mcu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>675</td>\n",
       "      <td>35</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>694</td>\n",
       "      <td>30</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>695</td>\n",
       "      <td>30</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    frame_file  cluster  shot_id  mcu\n",
       "75         675       35       17    0\n",
       "94         694       30       24    0\n",
       "95         695       30       24    1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# earliest frame with an Anchor cluster\n",
    "scene_df.loc[(scene_df['cluster'] == pair[0]) | (scene_df['cluster'] == pair[1])].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame_file</th>\n",
       "      <th>cluster</th>\n",
       "      <th>shot_id</th>\n",
       "      <th>mcu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>755</td>\n",
       "      <td>30</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>756</td>\n",
       "      <td>35</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>757</td>\n",
       "      <td>30</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     frame_file  cluster  shot_id  mcu\n",
       "155         755       30       41    1\n",
       "156         756       35       42    1\n",
       "157         757       30       43    1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# last frame with an Anchor cluster\n",
    "scene_df.loc[(scene_df['cluster'] == pair[0]) | (scene_df['cluster'] == pair[1])].tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675 757\n"
     ]
    }
   ],
   "source": [
    "anchor_start = scene_df.loc[(scene_df['cluster'] == pair[0]) | (scene_df['cluster'] == pair[1])].frame_file.min()\n",
    "anchor_end = scene_df.loc[(scene_df['cluster'] == pair[0]) | (scene_df['cluster'] == pair[1])].frame_file.max()\n",
    "print(anchor_start, anchor_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Identifying cutaways\n",
    "With the Anchor start and end frames, we have a preliminary idea of where the scene starts and ends. However, we should look at all of the other shots (clusters) between the Anchor start and end frames. These clusters represent cutaways, which can include the following:\n",
    "- POV shots, showing what characters are looking at offscreen\n",
    "- Inserts, different shots of Speaker A or B, such as a one-off close-up\n",
    "- Other characters, both silent and speaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([31, 14,  5, 17, 10,  4, 25,  1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cutaways = scene_df.loc[(scene_df['frame_file'] > anchor_start) & (scene_df['frame_file'] < anchor_end)].cluster.unique()\n",
    "cutaways = cutaways[cutaways != pair[0]] # remove the Speaker A and Speaker B clusters from this list\n",
    "cutaways = cutaways[cutaways != pair[1]]\n",
    "cutaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Expanding the scene's beginning and end using cutaways\n",
    "After we identify these cutaways, we may be able to expand the scene's start frame backward, and the end frame forward. If we see these cutaways again, but before the Anchor start or after the Anchor end, they must still be part of the scene. In the interest of caution, we will only look for cutaways that are adjacent to the Anchor frames.\n",
    "\n",
    "Beginning with the Anchor's start frame, we look at the previous frame (which currently isn't designated part of the scene). If that frame's cluster value is in the cutaway lists, we include it in the scene and continue backwards. This continues until we encounter a frame which isn't a cutaway. We repeat this for the Anchor's end frame, this time progressing forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "648"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scene_start = anchor_start\n",
    "min_flag = 0\n",
    "\n",
    "while min_flag == 0:\n",
    "    try:\n",
    "        if int(scene_df.loc[scene_df['frame_file'] == (scene_start - 1)].cluster) in cutaways:\n",
    "            scene_start -= 1\n",
    "        else:\n",
    "            min_flag = 1\n",
    "    except TypeError: # error if hitting the beginning of the frame list\n",
    "        min_flag = 1\n",
    "scene_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "760"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scene_end = anchor_end\n",
    "max_flag = 0\n",
    "while max_flag == 0:\n",
    "    try:\n",
    "        if int(scene_df.loc[scene_df['frame_file'] == (scene_end + 1)].cluster) in cutaways:\n",
    "            scene_end += 1\n",
    "        else:\n",
    "            max_flag = 1\n",
    "    except TypeError: # error if hitting the end of the frame list\n",
    "        max_flag = 1 \n",
    "scene_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The beginning of the scene was expanded by 27 frames, and the ending by 3 frames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using functions\n",
    "The above functionality is replicated below, this time using functions. For this example, we'll be looking at 400 frames from *Extremely Wicked, Shockingly Evil and Vile*. The functions which can be found further below, at the end of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6603 images in the folder\n",
      "Selected 400 of those frames\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Number of clusters: 18\n"
     ]
    }
   ],
   "source": [
    "film = 'extremely_wicked'\n",
    "frame_choice = list(range(650, 1050))\n",
    "threshold = 3000\n",
    "\n",
    "dialogue_folder = os.path.join('dialogue_frames', film)\n",
    "print('There are', len(os.listdir(dialogue_folder)), 'images in the folder')\n",
    "print('Selected', len(frame_choice), 'of those frames')\n",
    "\n",
    "hac_labels = label_clusters(dialogue_folder, frame_choice, film, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only necessary if this wasn't run in the previous example\n",
    "# tuned_model = models.load_model('saved_models/tuned_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_values = predict_mcu(dialogue_folder, tuned_model, frame_choice, film)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "shot_id_list = get_shot_ids(frame_choice, hac_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame_file</th>\n",
       "      <th>cluster</th>\n",
       "      <th>shot_id</th>\n",
       "      <th>mcu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>650</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>651</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>652</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   frame_file  cluster  shot_id  mcu\n",
       "0         650        2        0    1\n",
       "1         651        2        0    1\n",
       "2         652        2        0    1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scene_df = pd.DataFrame(zip(frame_choice, hac_labels, shot_id_list, y_pred_values), columns=['frame_file', 'cluster', 'shot_id', 'mcu'])\n",
    "scene_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 2], [9, 13], [0, 1], [5, 10], [0, 6]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alternating_pairs = get_alternating_pairs(frame_choice, hac_labels, y_pred_values, shot_id_list)\n",
    "alternating_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster\t count\t mcu probability\n",
      "0 \t 165 \t 48.48%\n",
      "2 \t 61 \t 65.57%\n",
      "Fails MCU check\n",
      "\n",
      "9 \t 28 \t 96.43%\n",
      "13 \t 21 \t 100.00%\n",
      "Passes MCU check\n",
      "\n",
      "0 \t 165 \t 48.48%\n",
      "1 \t 9 \t 66.67%\n",
      "Fails MCU check\n",
      "\n",
      "5 \t 23 \t 95.65%\n",
      "10 \t 17 \t 100.00%\n",
      "Passes MCU check\n",
      "\n",
      "0 \t 165 \t 48.48%\n",
      "6 \t 9 \t 55.56%\n",
      "Fails MCU check\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[9, 13], [5, 10]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_pairs = mcu_check(alternating_pairs, scene_df)\n",
    "speaker_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The anchor_scenes() and expand_scenes() functions are to be used separately. anchor_scenes() only returns the Anchor frames of each scene, while expand_scenes() does this and also tries to expand the scene, returning Expansion frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker A and B Clusters: [9, 13]\n",
      "Anchor Start/End Frames: 690 743\n",
      "\n",
      "Speaker A and B Clusters: [5, 10]\n",
      "Anchor Start/End Frames: 954 997\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(690, 743), (954, 997)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors = anchor_scenes(speaker_pairs, scene_df)\n",
    "anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker A and B Clusters: [9, 13]\n",
      "Anchor Start/End Frames: 690 743\n",
      "Cutaway Clusters: [0]\n",
      "Expanded Start/End Frames: 690 745\n",
      "\n",
      "Speaker A and B Clusters: [5, 10]\n",
      "Anchor Start/End Frames: 954 997\n",
      "Cutaway Clusters: [16]\n",
      "Expanded Start/End Frames: 954 997\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(690, 745), (954, 997)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenes = expand_scenes(speaker_pairs, scene_df)\n",
    "scenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_clusters(dialogue_folder, frame_choice, film, threshold):\n",
    "\n",
    "    model = VGG16(weights='imagenet', include_top=False)\n",
    "    model.summary()\n",
    "\n",
    "    vgg16_feature_list = []\n",
    "\n",
    "    for x in frame_choice:\n",
    "        img_path = dialogue_folder + '/' + film + '_frame'+ str(x) + '.jpg'\n",
    "        img = image.load_img(img_path, target_size=(256, 256))\n",
    "        img_data = image.img_to_array(img)\n",
    "        img_data = np.expand_dims(img_data, axis=0)\n",
    "        img_data = preprocess_input(img_data)\n",
    "\n",
    "        vgg16_feature = model.predict(img_data)\n",
    "        vgg16_feature_np = np.array(vgg16_feature)\n",
    "        vgg16_feature_list.append(vgg16_feature_np.flatten())\n",
    "\n",
    "        x += 1\n",
    "\n",
    "    vgg16_feature_list_np = np.array(vgg16_feature_list)\n",
    "    vgg16_feature_list_np.shape\n",
    "\n",
    "    hac = AgglomerativeClustering(n_clusters = None, distance_threshold = threshold).fit(vgg16_feature_list_np)\n",
    "    hac_labels = hac.labels_\n",
    "    print('Number of clusters:', hac.n_clusters_)\n",
    "\n",
    "    return hac_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mcu(dialogue_folder, model, frame_choice, film):\n",
    "    image_list = []\n",
    "    for x in frame_choice:\n",
    "        image_list.append(img_to_array(load_img(dialogue_folder + '/' + film + '_frame'+ str(x) + '.jpg', target_size = (128, 128), color_mode = 'grayscale')))\n",
    "\n",
    "    image_array = np.array(image_list)\n",
    "    y_pred = model.predict_classes(image_array)\n",
    "\n",
    "    # the model's predict_classes method creates a NumPy array of arrays; this converts it to a list of 0/1 integers\n",
    "    y_pred_values = []\n",
    "    for prediction in y_pred:\n",
    "        y_pred_values.append(prediction[0])\n",
    "        \n",
    "    return y_pred_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shot_ids(frame_choice, hac_labels):\n",
    "    shot_id = 0\n",
    "    shot_id_list = []\n",
    "    prev_frame = 1000\n",
    "\n",
    "    for frame_file, cluster in zip(frame_choice, hac_labels):\n",
    "        if cluster != prev_frame and prev_frame != 1000:\n",
    "            shot_id += 1\n",
    "        shot_id_list.append(shot_id)\n",
    "        prev_frame = cluster\n",
    "    \n",
    "    return shot_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alternating_pairs(frame_choice, hac_labels, y_pred_values, shot_id_list):\n",
    "    \n",
    "    # to check for an A/B/A/B pattern, we must store the previous three clusters in memory\n",
    "    prev_clust_1 = 1001\n",
    "    prev_clust_2 = 1002\n",
    "    prev_clust_3 = 1003\n",
    "    prev_shot_id = -1\n",
    "    alternate_a_list = []\n",
    "    alternate_b_list = []\n",
    "\n",
    "    # zip our various lists into a usable data structure\n",
    "    for frame_file, cluster, mcu_flag, shot_id in zip(frame_choice, hac_labels, y_pred_values, shot_id_list):\n",
    "        # when iterating through each frame, look for an A/B/A/B pattern, and save the clusters of any patterns\n",
    "        if cluster == prev_clust_2 and prev_clust_1 == prev_clust_3:\n",
    "            alternate_a_list.append(min(cluster, prev_clust_1)) # min and max are used to avoid duplicates of (1, 2), (2, 1)\n",
    "            alternate_b_list.append(max(cluster, prev_clust_1))\n",
    "\n",
    "        # we use prev_shot_id to identify when there's a new shot (when the cluster value changes)\n",
    "        # every time there's a new shot, we update the cluster memory\n",
    "        if shot_id != prev_shot_id:\n",
    "            prev_shot_id = shot_id\n",
    "            prev_clust_3 = prev_clust_2\n",
    "            prev_clust_2 = prev_clust_1\n",
    "            prev_clust_1 = cluster\n",
    "\n",
    "    # save unique alternating pairs\n",
    "    alternating_pairs = []\n",
    "    for a, b, in zip(alternate_a_list, alternate_b_list):\n",
    "        if [int(a), int(b)] not in alternating_pairs:\n",
    "            alternating_pairs.append([int(a), int(b)])\n",
    "        \n",
    "    return alternating_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcu_check(alternating_pairs, scene_df):\n",
    "    \n",
    "    speaker_pairs = []\n",
    "    print('cluster\\t', 'count\\t', 'mcu probability')\n",
    "    \n",
    "    for pair in alternating_pairs:\n",
    "        # calculate the mean of each cluster's MCU column\n",
    "        mean_a = scene_df.loc[scene_df['cluster'] == pair[0]]['mcu'].mean()\n",
    "        mean_b = scene_df.loc[scene_df['cluster'] == pair[1]]['mcu'].mean()\n",
    "        print(pair[0], '\\t', scene_df.loc[scene_df['cluster'] == pair[0]]['mcu'].count(), '\\t', '{0:.2f}%'.format(mean_a * 100))\n",
    "        print(pair[1], '\\t', scene_df.loc[scene_df['cluster'] == pair[1]]['mcu'].count(), '\\t', '{0:.2f}%'.format(mean_b * 100))\n",
    "        \n",
    "        # an alternating pair will pass the MCU check if BOTH clusters have a MCU mean greater than .5\n",
    "        if mean_a > .5 and mean_b > .5:\n",
    "            print('Passes MCU check')\n",
    "            speaker_pairs.append(pair)\n",
    "        else:\n",
    "            print('Fails MCU check')\n",
    "        print()\n",
    "    \n",
    "    return speaker_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anchor_scenes(speaker_pairs, scene_df):\n",
    "    \n",
    "    anchor_scenes = []\n",
    "\n",
    "    for pair in speaker_pairs:\n",
    "        # designate the first and last frames with either Speaker A or Speaker B clusters as Anchors\n",
    "        anchor_start = scene_df.loc[(scene_df['cluster'] == pair[0]) | (scene_df['cluster'] == pair[1])].frame_file.min()\n",
    "        anchor_end = scene_df.loc[(scene_df['cluster'] == pair[0]) | (scene_df['cluster'] == pair[1])].frame_file.max()\n",
    "\n",
    "        print('Speaker A and B Clusters:', pair)\n",
    "        print('Anchor Start/End Frames:', anchor_start, anchor_end)\n",
    "        print()\n",
    "        anchor_scenes.append((anchor_start, anchor_end))\n",
    "        \n",
    "    return anchor_scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_scenes(speaker_pairs, scene_df):\n",
    "    \n",
    "    expanded_scenes = []\n",
    "\n",
    "    for pair in speaker_pairs:\n",
    "        # designate the first and last frames with either Speaker A or Speaker B clusters as Anchors\n",
    "        anchor_start = scene_df.loc[(scene_df['cluster'] == pair[0]) | (scene_df['cluster'] == pair[1])].frame_file.min()\n",
    "        anchor_end = scene_df.loc[(scene_df['cluster'] == pair[0]) | (scene_df['cluster'] == pair[1])].frame_file.max()\n",
    "        # find all unique clusters between the anchor_start and anchor_end frames\n",
    "        cutaways = scene_df.loc[(scene_df['frame_file'] > anchor_start) & (scene_df['frame_file'] < anchor_end)].cluster.unique()\n",
    "        cutaways = cutaways[cutaways != pair[0]] # remove the Speaker A and Speaker B clusters from this list\n",
    "        cutaways = cutaways[cutaways != pair[1]]\n",
    "        print('Speaker A and B Clusters:', pair)\n",
    "        print('Anchor Start/End Frames:', anchor_start, anchor_end)\n",
    "        print('Cutaway Clusters:', cutaways)\n",
    "\n",
    "        scene_start = anchor_start\n",
    "        min_flag = 0\n",
    "\n",
    "        # expand \n",
    "        while min_flag == 0:\n",
    "            try:\n",
    "                if int(scene_df.loc[scene_df['frame_file'] == (scene_start - 1)].cluster) in cutaways:\n",
    "                    scene_start -= 1\n",
    "                else:\n",
    "                    min_flag = 1\n",
    "            except TypeError: # error if hitting the beginning of the frame list\n",
    "                min_flag = 1\n",
    "\n",
    "        scene_end = anchor_end\n",
    "        max_flag = 0\n",
    "        while max_flag == 0:\n",
    "            try:\n",
    "                if int(scene_df.loc[scene_df['frame_file'] == (scene_end + 1)].cluster) in cutaways:\n",
    "                    scene_end += 1\n",
    "                else:\n",
    "                    max_flag = 1\n",
    "            except TypeError: # error if hitting the end of the frame list\n",
    "                max_flag = 1\n",
    "        \n",
    "        print('Expanded Start/End Frames:', scene_start, scene_end)\n",
    "        print()\n",
    "        expanded_scenes.append((scene_start, scene_end))\n",
    "            \n",
    "    return expanded_scenes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (moviegoer)",
   "language": "python",
   "name": "moviegoer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
